{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import csv\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import time, os\n",
    "\n",
    "def getNewsContent(htmlstring):\n",
    "    revision_date = \"\"\n",
    "    title = \"\"\n",
    "    keywords = \"\"\n",
    "    sector = \"\"\n",
    "    article = \"\"\n",
    "    \n",
    "    try_get_news_content_count = 0\n",
    "    while True:\n",
    "        try:\n",
    "            r = urllib.urlopen(htmlstring).read()\n",
    "            soup = BeautifulSoup(r)\n",
    "    \n",
    "            # These function use re.compile is too slow.\n",
    "            # dateTag = soup.find_all(\"div\", attrs={\"class\": re.compile(\"ArticleHeader_date.\")})\n",
    "            # headerTag = soup.find_all(\"h1\", attrs={\"class\": re.compile(\"ArticleHeader_headline.\")})\n",
    "    \n",
    "            dateTag = soup.find(\"meta\", attrs={\"name\":\"REVISION_DATE\"}) # this is UTC time\n",
    "            revision_date = dateTag['content'].encode('utf-8')\n",
    "\n",
    "            titleTag = soup.find(\"meta\", attrs={\"name\":\"sailthru.title\"})\n",
    "            title = titleTag['content'].encode('utf-8')\n",
    "\n",
    "            keywordsTag = soup.find(\"meta\", attrs={\"name\":\"keywords\"})\n",
    "            keywords = keywordsTag['content'].encode('utf-8')\n",
    "\n",
    "            sectorTag = soup.find(\"meta\", attrs={\"property\":\"og:article:section\"})\n",
    "            sector = sectorTag['content'].encode('utf-8')\n",
    "\n",
    "            articleTag = soup.find_all(\"div\", attrs={\"class\": re.compile(\"ArticleBody_body.\")})\n",
    "            article = articleTag[0].text.encode('utf-8').replace('\\n',' ').replace('\\t',' ').replace('\\r',' ').replace('\\\"',' ')\n",
    "            break                    \n",
    "        except:\n",
    "            print(\"Error in getNewsContent try_get_news_content_count = \"+str(try_get_news_content_count))\n",
    "            if try_get_news_content_count <= 3:\n",
    "                time.sleep(10)\n",
    "                try_get_news_content_count += 1\n",
    "            else:\n",
    "                print(\"URL Does Not Work in getNewsContent! \"+htmlstring)\n",
    "                break\n",
    "                \n",
    "    return revision_date, title, keywords, sector, article \n",
    "\n",
    "# for archive pages only\n",
    "def getUrlsFromArchiveByDate(date):\n",
    "    archivePageByDate = \"http://www.reuters.com/resources/archive/us/\"+date+\".html\"\n",
    "    page = urllib.urlopen(archivePageByDate).read()\n",
    "    soup = BeautifulSoup(page)\n",
    "\n",
    "    moduleBody = soup.find_all(\"div\", attrs={\"class\":\"headlineMed\"})\n",
    "\n",
    "    url_list = []\n",
    "    for i in range(len(moduleBody)):\n",
    "        url = moduleBody[i].a[\"href\"]\n",
    "        if (\"/news/picture/\" in url) or (\"/news/video/\" in url) or (\"article/pictures-report\" in url) or (\"/article/life-\" in url):\n",
    "            continue\n",
    "        else:\n",
    "            url_list.append(url)\n",
    "    return url_list\n",
    "\n",
    "def convertTimestamp(time_str):\n",
    "    if len(time_str) == 0:\n",
    "       return \"\"\n",
    "\n",
    "    time_arr = time_str.split(\" \")\n",
    "    year_str = time_arr[5]\n",
    "    time_zone = time_arr[4]\n",
    "    minute = time_arr[3][:-3]\n",
    "    day_str = time_arr[2]\n",
    "    month_str = time_arr[1]\n",
    "\n",
    "    time_to_convert = month_str+' '+day_str+' '+year_str+' '+minute+' '+time_zone\n",
    "\n",
    "    utc_datetime_object = datetime.strptime(time_to_convert,'%b %d %Y %H:%M %Z')\n",
    "    adjusted_EST_time = utc_datetime_object - timedelta(hours=4)\n",
    "    return str(adjusted_EST_time)\n",
    "\n",
    "def getCsvFileByDate(date):\n",
    "    urls = getUrlsFromArchiveByDate(date)\n",
    "    print 'Number of URLs for ' + date + ' : ' + str(len(urls))\n",
    "    count = 0\n",
    "    with open(date+\".json\", \"wb\") as csv_file:\n",
    "        csv_file.write('[\\n')\n",
    "        for url in urls:   \n",
    "            \n",
    "            time_web, title, keywords, sector, article = getNewsContent(url)\n",
    "\n",
    "            adjusted_time = convertTimestamp(time_web)\n",
    "            adjusted_time = \"\\\"news_time\\\":\\\"\" + adjusted_time + \"\\\"\"\n",
    "            title = \"\\\"news_title\\\":\\\"\" + title + \"\\\"\"\n",
    "            keywords = \"\\\"keywords\\\":\\\"\" + keywords + \"\\\"\"\n",
    "            sector = \"\\\"sector\\\":\\\"\" + sector + \"\\\"\"\n",
    "            article = \"\\\"content\\\":\\\"\" + article + \"\\\"\"\n",
    "            url_encoded = \"\\\"url\\\":\\\"\" + url.encode('utf-8') + \"\\\"\"\n",
    "            line = '{' + adjusted_time + ',' + title + ',' + keywords + ',' + sector + ',' + article + ',' + url_encoded + '}'\n",
    "\n",
    "            csv_file.write(line)\n",
    "            csv_file.write('\\n')\n",
    "            count += 1\n",
    "            print(\"Finished \"+str(count)+\" - \"+url)\n",
    "\n",
    "        csv_file.write(']')\n",
    "        \n",
    "    print('Completed! Total number of scrapped url from '+date+' is '+str(count))\n",
    "\n",
    "dates = ['20170712']\n",
    "for date in dates:\n",
    "     getCsvFileByDate(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python27_env]",
   "language": "python",
   "name": "conda-env-python27_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
